<!DOCTYPE html> <html lang="en, cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yu Mei </title> <meta name="author" content="Yu Mei"> <meta name="description" content="Yu Mei's academic homepage. "> <meta name="keywords" content="Yu Mei, robotics, soft robotics, control theory, robot learning, flexible actuators and sensors"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Spartan_Helmet_Green.png?v=4d3ecd99a0566f7898fdb57c6becb33f"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yu-mei.github.io/"> <script src="/assets/js/theme.js?v=6cf204b852c8b4c5693e4c311b520c85"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="/assets/pdf/example_pdf.pdf" title="Cv pdf"><i class="ai ai-cv"></i></a> <a href="mailto:you@example.com" title="Email"><i class="fa-solid fa-envelope"></i></a> <a href="https://inspirehep.net/authors/1010907" title="Inspirehep id" rel="external nofollow noopener" target="_blank"><i class="ai ai-inspire"></i></a> <a href="/feed.xml" title="Rss icon"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Scholar userid" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.alberteinstein.com/" title="Custom Social" rel="external nofollow noopener" target="_blank"><img src="https://www.alberteinstein.com/wp-content/uploads/2024/03/cropped-favicon-192x192.png" alt="Custom Social"></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yu</span> Mei </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/YuMei_800-480.webp 480w,/assets/img/YuMei_800-800.webp 800w,/assets/img/YuMei_800-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/YuMei_800.jpg?v=a3d472df2d7a521c50fa762abfce7e7e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="YuMei_800.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am a PhD student in the <a href="https://smlab.msu.edu/" rel="external nofollow noopener" target="_blank">Smart Microsystems Lab</a> at <a href="https://msu.edu/" rel="external nofollow noopener" target="_blank">Michigan State University</a>, advised by <a href="https://www.egr.msu.edu/~xbtan/" rel="external nofollow noopener" target="_blank">Dr. Xiaobo Tan</a> starting from 2021. Prior to joining MSU, I received my Bachelor degree in Robotics Engineering from the Department of Automation at <a href="https://www.seu.edu.cn/english/" rel="external nofollow noopener" target="_blank">Southeast University</a> in 2020, advised by <a href="https://scholar.google.com/citations?user=RjQ5TrEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Dr. Aiguo Song</a>.</p> <p>My research focuses on soft robotics, control systems and mechatronics, with an emphasis on soft actuator and sensor system design, as well as data-driven and learning-based modeling and control of robotic systems.</p> <p>I was recognized as an <a href="https://sites.gatech.edu/mechatronics/dscdrisingstars/" rel="external nofollow noopener" target="_blank">ASME Dynamic Systems and Control Division (DSCD) Rising Star</a> in 2025. My work received the <a href="https://smlab.msu.edu/news/yu-mei-received-aim-2025-best-student-paper-award/#:~:text=This%20work%2C%20%E2%80%9CSimultaneous%20shape%20reconstruction,Vaibhav%20Srivastava%2C%20and%20Xiaobo%20Tan." rel="external nofollow noopener" target="_blank">Best Student Paper Award</a> at the IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM) in 2024, and the <a href="https://mecc2025.a2c2.org/student-best-paper/" rel="external nofollow noopener" target="_blank">Best Student Paper Finalist</a> at the Modeling, Estimation and Control Conference (MECC) in 2025.</p> </div> <style>.news-link{text-decoration:none;color:inherit;font-size:2em;margin-right:10px}.news-link:hover{text-decoration:underline}.news-divider{flex-grow:1;height:1px;background-color:#ccc}.news table.table td,.news table.table th{padding-top:.5rem;padding-bottom:.5rem;line-height:1.1}.news td p{margin:0}</style> <div style="display: flex; align-items: center; margin: 20px 0 10px;"> <a href="/news/" class="news-link">News</a> <div class="news-divider"></div> </div> <div class="news"> <div class="table-responsive" style="max-height: 30vw;"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct. 2025</th> <td> <img class="emoji" title=":medal_military:" alt=":medal_military:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f396.png" height="20" width="20"> I am honored to be selected as <a href="https://sites.gatech.edu/mechatronics/dscdrisingstars/" rel="external nofollow noopener" target="_blank">ASME Dynamic Systems and Control Division (DSCD) Rising Star</a> and give an invited talk at <a href="https://mecc2025.a2c2.org/" rel="external nofollow noopener" target="_blank">MECC 2025</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct. 2025</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2504.18692" rel="external nofollow noopener" target="_blank">learning-based modeling of soft actuators</a> is selected as a <a class="award-link" href="https://mecc2025.a2c2.org/student-best-paper/" rel="external nofollow noopener" target="_blank">Best Student Paper Finalist</a> at <a href="https://mecc2025.a2c2.org/" rel="external nofollow noopener" target="_blank">MECC 2025</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul. 2025</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> Two papers on <a href="https://arxiv.org/abs/2504.16369" rel="external nofollow noopener" target="_blank">adaptive neural MPC via meta-learning</a> and <a href="https://arxiv.org/abs/2504.18692" rel="external nofollow noopener" target="_blank">learning-based modeling of soft actuators</a> are accepted to <a href="https://mecc2025.a2c2.org/" rel="external nofollow noopener" target="_blank">MECC 2025</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug. 2024</th> <td> <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper on <a href="https://ieeexplore.ieee.org/abstract/document/10536011" rel="external nofollow noopener" target="_blank">simultaneous shape reconstruction and force estimation of soft robots</a> received the <a class="award-link" href="https://smlab.msu.edu/news/yu-mei-received-aim-2025-best-student-paper-award/" rel="external nofollow noopener" target="_blank">Best Student Paper Award</a> at <a href="https://ras.papercept.net/images/temp/AIM/AIM24_WelcomeMedia.html" rel="external nofollow noopener" target="_blank">AIM 2024</a>, and was covered by <a href="https://www.linkedin.com/posts/msuegr_congratulations-to-yu-mei-phd-student-in-activity-7231676404558434304-2UIs/" rel="external nofollow noopener" target="_blank">MSU College of Engineering News</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">May. 2024</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> One paper on <a href="https://ieeexplore.ieee.org/abstract/document/10536011" rel="external nofollow noopener" target="_blank">simultaneous shape reconstruction and force estimation of soft robots</a> is accepted to <a href="http://www.ieee-asme-mechatronics.info/" rel="external nofollow noopener" target="_blank">IEEE/ASME TMech</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan. 2023</th> <td> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> One paper on <a href="https://ieeexplore.ieee.org/document/10156049" rel="external nofollow noopener" target="_blank">simultaneous motion and stiffness control of soft robots</a> is accepted to <a href="https://acc2023.a2c2.org/" rel="external nofollow noopener" target="_blank">ACC 2023</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug. 2021</th> <td> I joined the <a href="https://smlab.msu.edu/" rel="external nofollow noopener" target="_blank">Smart Microsystems Lab</a> at <a href="https://msu.edu/" rel="external nofollow noopener" target="_blank">Michigan State University</a> with the <a href="https://engineering.msu.edu/current-students/graduate/funding-opportunities" rel="external nofollow noopener" target="_blank">Engineering Distinguished Scholar</a> Fellowship. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct. 2020</th> <td> I ioined the Department of Automation at Southeast University as a Research Assistant, mentored by <a href="https://www.researchgate.net/profile/Changyin-Sun" rel="external nofollow noopener" target="_blank">Dr. Changyin Sun</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun. 2020</th> <td> I graduated from <a href="https://www.seu.edu.cn/english/" rel="external nofollow noopener" target="_blank">Southeast University</a> (B.Eng.) with the Outstanding Graduate honor. </td> </tr> </table> </div> </div> <style>.selected-link{text-decoration:none;color:inherit;font-size:2em;margin-right:10px}.selected-link:hover{text-decoration:underline}.selected-divider{flex-grow:1;height:1px;background-color:#ccc}</style> <div style="display: flex; align-items: center; margin: 30px 0 10px;"> <a href="/publications/" class="selected-link">Selected Publications</a> <div class="selected-divider"></div> </div> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/mlmp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mlmp.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="noori2025test" class="col-sm-8"> <div class="title">Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</div> <div class="author"> <em>Yu Mei</em>, Gustavo Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In NeurIPS</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.21844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, seven segmentation datasets, and 15 common corruptions, with a total of 82 distinct test scenarios, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/fds.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fds.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="noori2025fds" class="col-sm-8"> <div class="title">FDS: Feedback-Guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization</div> <div class="author"> Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In WACV</em>, 2025 - <span class="award-text">ETS Research Award <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"></span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.03588" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>ETS Research Award</p> </div> <div class="abstract hidden"> <p>Domain Generalization techniques aim to enhance model robustness by simulating novel data distributions during training, typically through various augmentation or stylization strategies. However, these methods frequently suffer from limited control over the diversity of generated images and lack assurance that these images span distinct distributions. To address these challenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategy that employs diffusion models to synthesize novel, pseudo-domains by training a single model on all source domains and performing domain mixing based on learned features. By incorporating images that pose classification challenges to models trained on original samples, alongside the original dataset, we ensure the generation of a training set that spans a broad distribution spectrum. Our comprehensive evaluations demonstrate that this methodology sets new benchmarks in domain generalization performance across a range of challenging datasets, effectively managing diverse types of domain shifts. The code can be found at: https://github.com/Mehrdad-Noori/FDS.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/svwa_tta.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="svwa_tta.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bahri2025tesu" class="col-sm-8"> <div class="title">Test-Time Adaptation in Point Clouds: Leveraging Sampling Variation with Weight Averaging</div> <div class="author"> Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Farzad Beizaee, Gustavo Hakim, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In WACV</em>, 2025 - <span class="award-text">Oral <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"></span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.01116" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral</p> </div> <div class="abstract hidden"> <p>Test-Time Adaptation (TTA) addresses distribution shifts during testing by adapting a pretrained model without access to source data. In this work, we propose a novel TTA approach for 3D point cloud classification, combining sampling variation with weight averaging. Our method leverages Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN) to create multiple point cloud representations, adapting the model for each variation using the TENT algorithm. The final model parameters are obtained by averaging the adapted weights, leading to improved robustness against distribution shifts. Extensive experiments on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C datasets, with different backbones (Point-MAE, Point-Net, DGCNN), demonstrate that our approach consistently outperforms existing methods while maintaining minimal resource overhead. The proposed method effectively enhances model generalization and stability in challenging real-world conditions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/simamba.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="simamba.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bahri2025spectral" class="col-sm-8"> <div class="title">Spectral Informed Mamba for Robust Point Cloud Processing</div> <div class="author"> Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Hakim, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In CVPR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2503.04953" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>State space models have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba’s capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in Masked Autoencoder for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate the improvements of our approach in classification, segmentation, and few-shot tasks over state-of-the-art baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/watt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="watt.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="osowiechi2024watt" class="col-sm-8"> <div class="title">WATT: Weight Average Test Time Adaptation of CLIP</div> <div class="author"> David Osowiechi<sup>*</sup>, Mehrdad Noori<sup>*</sup>, Gustavo Hakim, Moslem Yazdanpanah, Ali Bahri, Milad Cheraghalikhani, Sahar Dastani, Farzad Beizaee, Ismail Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.13875" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Vision-Language Models (VLMs) such as CLIP have yielded unprecedented performances for zero-shot image classification, yet their generalization capability may still be seriously challenged when confronted to domain shifts. In response, we present Weight Average Test-Time Adaptation (WATT) of CLIP, a new approach facilitating full test-time adaptation (TTA) of this VLM. Our method employs a diverse set of templates for text prompts, augmenting the existing framework of CLIP. Predictions are utilized as pseudo labels for model updates, followed by weight averaging to consolidate the learned information globally. Furthermore, we introduce a text ensemble strategy, enhancing the overall test performance by aggregating diverse textual cues.Our findings underscore the effectiveness of WATT across diverse datasets, including CIFAR-10-C, CIFAR-10.1, CIFAR-100-C, VisDA-C, and several other challenging datasets, effectively covering a wide range of domain shifts. Notably, these enhancements are achieved without the need for additional model transformations or trainable modules. Moreover, compared to other TTA methods, our approach can operate effectively with just a single image. The code is available at: https://github.com/Mehrdad-Noori/WATT.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/tfs.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tfs.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="noori2024tfs" class="col-sm-8"> <div class="title">Tfs-vit: Token-level feature stylization for domain generalization</div> <div class="author"> Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Hakim, David Osowiechi, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>Pattern Recognition</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.15698" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks. However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity. Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/ncttt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ncttt.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="osowiechi2024nc" class="col-sm-8"> <div class="title">Nc-ttt: A noise constrastive approach for test-time training</div> <div class="author"> David Osowiechi, Gustavo Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, and Christian Desrosiers </div> <div class="periodical"> <em>In CVPR</em>, 2024 - <span class="award-text">Highlight <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"></span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.08392" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Highlight</p> </div> <div class="abstract hidden"> <p>Despite their exceptional performance in vision tasks deep learning models often struggle when faced with domain shifts during testing. Test-Time Training (TTT) methods have recently gained popularity by their ability to enhance the robustness of models through the addition of an auxiliary objective that is jointly optimized with the main task. Being strictly unsupervised this auxiliary objective is used at test time to adapt the model without any access to labels. In this work we propose Noise-Contrastive Test-Time Training (NC-TTT) a novel unsupervised TTT technique based on the discrimination of noisy feature maps. By learning to classify noisy views of projected feature maps and then adapting the model accordingly on new domains classification performance can be recovered by an important margin. Experiments on several popular test-time adaptation baselines demonstrate the advantages of our method compared to recent approaches for this task. The code can be found at: https://github.com/GustavoVargasHakim/NCTTT.git</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/cagnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cagnet.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mohammadi2020cagnet" class="col-sm-8"> <div class="title">CAGNet: Content-aware guidance for salient object detection</div> <div class="author"> Sina Mohammadi<sup>*</sup>, Mehrdad Noori<sup>*</sup>, Ali Bahri, Sina Ghofrani Majelan, and Mohammad Havaei </div> <div class="periodical"> <em>Pattern Recognition</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1911.13168" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have “salient-like” appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have “salient-like” appearance; ii) assign foreground label to different-looking salient regions. Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on six challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics. Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480 × 480 image.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/dfnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dfnet.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="noori2020dfnet" class="col-sm-8"> <div class="title">DFNet: Discriminative feature extraction and integration network for salient object detection</div> <div class="author"> Mehrdad Noori<sup>*</sup>, Sina Mohammadi<sup>*</sup>, Sina Ghofrani Majelan, Ali Bahri, and Mohammad Havaei </div> <div class="periodical"> <em>Engineering Applications of Artificial Intelligence</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2004.01573" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/brain.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="brain.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="noori2019attention" class="col-sm-8"> <div class="title">Attention-guided version of 2D UNet for automatic brain tumor segmentation</div> <div class="author"> Mehrdad Noori, Ali Bahri, and Karim Mohammadi </div> <div class="periodical"> <em>In ICCKE</em>, 2019 - <span class="award-text">Best Paper <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"></span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2004.02009" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper</p> </div> <div class="abstract hidden"> <p>Gliomas are the most common and aggressive among brain tumors, which cause a short life expectancy in their highest grade. Therefore, treatment assessment is a key stage to enhance the quality of the patients’ lives. Recently, deep convolutional neural networks (DCNNs) have achieved a remarkable performance in brain tumor segmentation, but this task is still difficult owing to high varying intensity and appearance of gliomas. Most of the existing methods, especially UNet-based networks, integrate low-level and high-level features in a naive way, which may result in confusion for the model. Moreover, most approaches employ 3D architectures to benefit from 3D contextual information of input images. These architectures contain more parameters and computational complexity than 2D architectures. On the other hand, using 2D models causes not to benefit from 3D contextual information of input images. In order to address the mentioned issues, we design a low-parameter network based on 2D UNet in which we employ two techniques. The first technique is an attention mechanism, which is adopted after concatenation of low-level and high-level features. This technique prevents confusion for the model by weighting each of the channels adaptively. The second technique is the Multi-View Fusion. By adopting this technique, we can benefit from 3D contextual information of input images despite using a 2D model. Experimental results demonstrate that our method performs favorably against 2017 and 2018 state-of-the-art methods.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="/assets/pdf/example_pdf.pdf" title="Cv pdf"><i class="ai ai-cv"></i></a> <a href="mailto:you@example.com" title="Email"><i class="fa-solid fa-envelope"></i></a> <a href="https://inspirehep.net/authors/1010907" title="Inspirehep id" rel="external nofollow noopener" target="_blank"><i class="ai ai-inspire"></i></a> <a href="/feed.xml" title="Rss icon"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Scholar userid" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.alberteinstein.com/" title="Custom Social" rel="external nofollow noopener" target="_blank"><img src="https://www.alberteinstein.com/wp-content/uploads/2024/03/cropped-favicon-192x192.png" alt="Custom Social"></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yu Mei. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>